services:
  ## Setting up the LiteLLM instances
  litellm:
    container_name: litellm
    image: ghcr.io/berriai/litellm:main-latest
    ports:
      - 4000:4000 # Map the container port to the host, change the host port if necessary
    volumes:
      - ./litellm-config.yaml:/app/config.yaml # Mount the local configuration file
    # You can change the port or number of workers as per your requirements or pass any new supported CLI augument. Make sure the port passed here matches with the container port defined above in `ports` value
    command: [ "--config", "/app/config.yaml", "--port", "4000"]
    networks:
      - proxy
      - internal
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - DATABASE_URL=postgresql://${LITELLM_POSTGRES_USER:-postgres}:${LITELLM_POSTGRES_PASSWORD:-postgres}@${LITELLM_POSTGRES_HOST:-litellm_db}:${LITELLM_POSTGRES_PORT:-5432}/${LITELLM_POSTGRES_DATABASE:-postgres}
      - STORE_MODEL_IN_DB=True # allows adding models to proxy via UI
      - UI_USERNAME=${UI_USERNAME}
      - UI_PASSWORD=${UI_PASSWORD}
    env_file:
      - .env
    depends_on:
      - litellm_db

  litellm_db:
    image: postgres:16.1
    container_name: litellm_db
    restart: always
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=${LITELLM_POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${LITELLM_POSTGRES_PASSWORD:-postgres}
      - POSTGRES_PORT=${LITELLM_POSTGRES_PORT:-5432}
      - POSTGRES_DATABASE=${LITELLM_POSTGRES_DATABASE:-postgres}
      - POSTGRES_HOST=${LITELLM_POSTGRES_HOST:-litellm_db}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - internal

  ## Setting up for OpenWebUI instance
  openwebui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - 3000:8080
    volumes:
      - open-webui-local:/app/backend/data
      - ./data/docs:/data/docs
    environment:
      - OPENAI_API_BASE_URLS=${OPENAI_API_BASE_URLS}
      - OPENAI_API_KEYS=${OPENAI_API_KEYS}
      - ENABLE_LITELLM=True
      - LITELLM_PROXY_PORT=4000
      - LITELLM_PROXY_HOST=127.0.0.1
      ## apache tika
      - TIKA_SERVER_URL=http://tika:9998
      - CONTENT_EXTRACTION_ENGINE=tika
    env_file:
      - .env
    restart: unless-stopped
    networks:
      - proxy
      - internal

  tika:
    image: apache/tika:latest-full
    container_name: tika
    ports:
      - "9998:9998"
    restart: unless-stopped
    networks:
      - internal

  n8n:
    container_name: n8n
    image: n8nio/n8n
    restart: unless-stopped
    ports:
      - "5678:5678"
    volumes:
      - n8n_data:/home/node/.n8n
    environment:
      - N8N_HOST=localhost
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - WEBHOOK_URL=http://localhost:5678/
      - GENERIC_API_KEY=${N8N_GENERIC_API_KEY:-your_n8n_api_key} # Replace with a strong API key
      - NODE_FUNCTION_ALLOW_EXTERNAL=litellm:4000 # Allow N8N to connect to LiteLLM
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_BASIC_AUTH_USER:-n8nuser}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_BASIC_AUTH_PASSWORD:-n8npassword}
      
    networks:
      - internal

  qdrant:
    container_name: qdrant
    image: qdrant/qdrant
    restart: unless-stopped
    ports:
      - "6333:6333" # HTTP API
      - "6334:6334" # gRPC API
    volumes:
      - qdrant_data:/qdrant/data
    networks:
      - internal

networks:
  proxy:
    external: true
  internal:

volumes:
  postgres_data:
  open-webui-local:
  n8n_data:
  qdrant_data:
